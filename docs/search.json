[
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "",
    "text": "In this blog post, I describe an early attempt at performing live voice activity detection with pyannote.audio pretrained segmentation model."
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#rolling-audio-buffer",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#rolling-audio-buffer",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Rolling audio buffer",
    "text": "Rolling audio buffer\nLet us assume that the audio stream is given as a 5s rolling buffer.\nHere, we are going to fake it by sliding a 5s window over the duration of an audio file.\n\nfrom pyannote.audio.core.io import Audio, AudioFile\n\nclass RollingAudioBuffer(Audio):\n    \"\"\"Rolling audio buffer\n    \n    Parameters\n    ----------\n    sample_rate : int\n        Sample rate\n    duration : float, optional\n        Duration of rolling buffer. Defaults to 5s.\n    step : float, optional\n        Delay between two updates of the rolling buffer. Defaults to 1s.\n\n\n    Usage\n    -----\n    &gt;&gt;&gt; buffer = RollingAudioBuffer()(\"audio.wav\")\n    &gt;&gt;&gt; current_buffer = next(buffer)\n    \"\"\"\n    def __init__(self, sample_rate=16000, duration=5.0, step=1.):\n        super().__init__(sample_rate=sample_rate, mono=True)\n        self.duration = duration\n        self.step = step\n        \n    def __call__(self, file: AudioFile):\n        \n        # duration of the whole audio file\n        duration = self.get_duration(file)\n        \n        # slide a 5s window from the beginning to the end of the file\n        window = SlidingWindow(start=0., duration=self.duration, step=self.step, end=duration)\n        for chunk in window:\n            # for each position of the window, yield the corresponding audio buffer\n            # as a SlidingWindowFeature instance\n            waveform, sample_rate = self.crop(file, chunk, duration=self.duration)\n            resolution = SlidingWindow(start=chunk.start, \n                                       duration=1./self.sample_rate, \n                                       step=1./sample_rate)\n            yield SWF(waveform.T, resolution)\n\nWe start by initializing rolling buffer on a sample file:\n\nMY_AUDIO_FILE = \"DH_0001.flac\"\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\n\nEach subsequent call to next(buffer) returns the current content of the 5s rolling buffer:\n\nnext(buffer)\n\n\n\n\n\nnext(buffer)\n\n\n\n\n\nnext(buffer)\n\n\n\n\nFor illustration purposes, we also load the manual voice activity reference.\n\nfrom pyannote.database.util import load_rttm\nreference = load_rttm('DH_0001.rttm').popitem()[1].get_timeline()\nreference"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#pretrained-voice-activity-detection-model",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#pretrained-voice-activity-detection-model",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Pretrained voice activity detection model",
    "text": "Pretrained voice activity detection model\npyannote.audio comes with a decent pretrained segmentation model that can be used for voice activity detection.\n\nimport torch\nimport numpy as np\nfrom pyannote.audio import Model\n\nclass VoiceActivityDetection:\n    \n    def __init__(self):\n        self.model = Model.from_pretrained(\"pyannote/segmentation\")\n        self.model.eval()\n        \n    def __call__(self, current_buffer: SWF) -&gt; SWF:\n        \n        # we start by applying the model on the current buffer\n        with torch.no_grad():\n            waveform = current_buffer.data.T\n            segmentation = self.model(waveform[np.newaxis]).numpy()[0]\n\n        # temporal resolution of the output of the model\n        resolution = self.model.introspection.frames\n        \n        # temporal shift to keep track of current buffer start time\n        resolution = SlidingWindow(start=current_buffer.sliding_window.start, \n                                   duration=resolution.duration, \n                                   step=resolution.step)\n            \n        # pyannote/segmentation pretrained model actually does more than just voice activity detection\n        # see https://huggingface.co/pyannote/segmentation for more details.     \n        speech_probability = np.max(segmentation, axis=-1, keepdims=True)\n        \n        return SWF(speech_probability, resolution)\n\n\nvad = VoiceActivityDetection()\n\nLet us try this thing on current buffer:\n\ncurrent_buffer = next(buffer)\ncurrent_buffer\n\n\n\n\n\nvad(current_buffer)\n\n\n\n\n\nreference"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#building-a-basic-streaming-pipeline-with-streamz",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#building-a-basic-streaming-pipeline-with-streamz",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Building a basic streaming pipeline with streamz",
    "text": "Building a basic streaming pipeline with streamz\nWe now have a way to stream audio and apply voice activity detection.\nAccording to its documentation, streamz seems like a good option to do that:\n\nStreamz helps you build pipelines to manage continuous streams of data.\n\nLet us start by creating a Stream that will ingest the rolling buffer and apply voice activity detection anytime the buffer is updated.\n\nfrom streamz import Stream\nsource = Stream()\nsource.map(vad).sink(visualize)\n\n\n\n\nWe re-initialize the audio buffer from the start of the file and push the rolling buffer into the pipeline:\n\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\nsource.emit(next(buffer))\n\n\n\n\n\nsource.emit(next(buffer))\n\n\n\n\n\nsource.emit(next(buffer))\n\n\n\n\n\nreference"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#controlling-latency-accuracy-trade-off",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#controlling-latency-accuracy-trade-off",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Controlling latency / accuracy trade-off",
    "text": "Controlling latency / accuracy trade-off\nThis is nice but we can do better in case the pipeline is allowed a small delay (a.k.a. latency) between when it receives the audio and when it outputs the voice activity detection scores.\nFor instance, if we are allowed 2s latency, we could benefit from the multiple overlapping buffers and combine them to get a better estimate of the speech probability in regions where the model is not quite confident (e.g.Â just before t=4s).\nThis is what the Aggregation class does.\n\nfrom typing import Tuple, List\n\nclass Aggregation:\n    \"\"\"Aggregate multiple overlapping buffers with a \n    \n    Parameters\n    ----------\n    latency : float, optional\n        Allowed latency, in seconds. Defaults to 0.\n    \"\"\"\n    \n    def __init__(self, latency=0.0):\n        self.latency = latency\n        \n    def __call__(self, internal_state, current_buffer: SWF) -&gt; Tuple[Tuple[float, List[SWF]], SWF]:\n        \"\"\"Ingest new buffer and return aggregated output with delay\n\n        Parameters\n        ----------\n        internal_state : (internal_time, past_buffers) tuple\n            `internal_time` is a float such that previous call emitted aggregated scores up \n            to time `delayed_time`.  `past_buffers` is a rolling list of past buffers that \n            we are going to aggregate.\n        current_buffer : SlidingWindowFeature\n            New incoming score buffer.\n        \"\"\"\n\n        if internal_state is None:\n            internal_state = (0.0, list())\n        \n        # previous call led to the emission of aggregated scores up to time `delayed_time`\n        # `past_buffers` is a rolling list of past buffers that we are going to aggregate\n        delayed_time, past_buffers = internal_state\n        \n        # real time is the current end time of the audio buffer\n        # (here, estimated from the end time of the VAD buffer)\n        real_time = current_buffer.extent.end\n        \n        # because we are only allowed `self.latency` seconds of latency, this call should\n        # return aggregated scores for [delayed_time, real_time - latency] time range. \n        required = Segment(delayed_time, real_time - self.latency)\n        \n        # to compute more robust scores, we will combine all buffers that have a non-empty\n        # temporal intersection with required time range. we can get rid of the others as they\n        # will no longer be needed as they are too far away in the past.\n        past_buffers = [buffer for buffer in past_buffers if buffer.extent.end &gt; required.start] + [current_buffer]\n        \n        # we aggregate all past buffers (but only on the 'required' region of interest)\n        intersection = np.stack([buffer.crop(required, fixed=required.duration) for buffer in past_buffers])\n        aggregation = np.mean(intersection, axis=0)\n        \n        # ... and wrap it into a self-contained SlidingWindowFeature (SWF) instance\n        resolution = current_buffer.sliding_window\n        resolution = SlidingWindow(start=required.start, duration=resolution.duration, step=resolution.step)\n        output = SWF(aggregation, resolution)\n        \n        # we update the internal state\n        delayed_time = real_time - self.latency\n        internal_state = (delayed_time, past_buffers)\n        \n        # ... and return the whole thing for next call to know where we are\n        return internal_state, output\n\nLetâs add this new accumulator into the streaming pipeline, with a 2s latency:\n\nsource = Stream()\nsource \\\n    .map(vad) \\\n    .accumulate(Aggregation(latency=2.), returns_state=True, start=None) \\\n    .sink(visualize)\n\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\n\n\n# first [0s, 5s] buffer\ncurrent_buffer = next(buffer); current_buffer\n\n\n\n\n\n# ... and its corresponding output up to 3s (= 5s - latency)\nsource.emit(current_buffer)\n\n\n\n\n\n# next [1s, 6s] buffer\ncurrent_buffer = next(buffer); current_buffer\n\n\n\n\n\n# ... and its corresponding [3s, 6s - latency = 4s] output\nsource.emit(current_buffer)\n\n\n\n\n\n# next [2s, 7s] buffer\ncurrent_buffer = next(buffer); current_buffer\n\n\n\n\n\n# ... and its corresponding [4s, 7s - latency = 5s] output\nsource.emit(current_buffer)\n\n\n\n\nLook how the aggregation process actually refined the speech probability just before t=4s. This has been enabled by the longer latency."
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#thats-all-folks",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#thats-all-folks",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Thatâs all folks!",
    "text": "Thatâs all folks!\nFor technical questions and bug reports, please check pyannote.audio Github repository.\nFor commercial enquiries and scientific consulting, please contact me."
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#bonus-concatenating-output",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#bonus-concatenating-output",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Bonus: concatenating output",
    "text": "Bonus: concatenating output\nFor visualization purposes, you might want to add an accumulator to the pipeline that takes care of concatenating the output of each stepâ¦\n\nclass Concatenation:\n    \n    def __call__(self, concatenation: SWF, current_buffer: SWF) -&gt; Tuple[SWF, SWF]:\n        \n        if concatenation is None:\n            return current_buffer, current_buffer\n        \n        resolution = concatenation.sliding_window\n        current_start_frame = resolution.closest_frame(current_buffer.extent.start)\n        current_end_frame = current_start_frame + len(current_buffer)\n        \n        concatenation.data = np.pad(concatenation.data, ((0, current_end_frame - len(concatenation.data)), (0, 0)))\n        concatenation.data[current_start_frame: current_end_frame] = current_buffer.data        \n        \n        return concatenation, concatenation\n\n\nsource = Stream()\nsource \\\n    .map(vad) \\\n    .accumulate(Aggregation(latency=2.), returns_state=True, start=None) \\\n    .accumulate(Concatenation(), returns_state=True, start=None) \\\n    .sink(visualize)\n\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\n\n\nnotebook.crop = Segment(0, 30)\nfor _ in range(30):\n    source.emit(next(buffer))\n\n\n\n\n\nreference"
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html",
    "title": "One speaker segmentation model to rule them all",
    "section": "",
    "text": "In this blog post, I talk about pyannote.audio pretrained speaker segmentation model, which happens to be one of the most popular audio model available on ð¤ Huggingface model hub.\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\"pyannote/segmentation\")"
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#what-does-pyannotesegmentation-do",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#what-does-pyannotesegmentation-do",
    "title": "One speaker segmentation model to rule them all",
    "section": "What does pyannote/segmentation do?",
    "text": "What does pyannote/segmentation do?\nEvery pyannote.audio model has a specifications attribute that tells us a bit more about itself:\n\nprint(model.specifications)\n\nSpecifications(\n    problem=&lt;Problem.MULTI_LABEL_CLASSIFICATION: 2&gt;,\n    resolution=&lt;Resolution.FRAME: 1&gt;,\n    duration=5.0,\n    warm_up=(0.0, 0.0),\n    classes=['speaker#1', 'speaker#2', 'speaker#3'],\n    permutation_invariant=True\n)\n\n\n\nThese specifications tell us the following about pyannote/segmentation:\n\nit ingests audio chunks of 5 seconds duration\nit addresses a multi-label classification problemâ¦\nâ¦ whose possible classes are chosen among speaker#1, speaker#2, and speaker#3 â¦\nâ¦ and are permutation_invariant (more about that below)\n\nWe also learn that its output temporal resolution is the frame (i.e. it outputs a sequence of frame-wise decisions rather than just one decision for the whole chunk). The actual temporal resolution can be obtained through the magic introspection attribute (approximately 17ms for pyannote/segmentation):\n\nmodel.introspection.frames.step\n\n0.016875"
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#ok-but-what-does-pyannotesegmentation-really-do",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#ok-but-what-does-pyannotesegmentation-really-do",
    "title": "One speaker segmentation model to rule them all",
    "section": "OK, but what does pyannote/segmentation really do?",
    "text": "OK, but what does pyannote/segmentation really do?\nTo answer this question, let us consider the audio recording of a 30s conversation between two speakers (the blue one and the red one):\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nLetâs apply the model on this 5s excerpt of the conversation:\n\nfrom pyannote.audio import Audio\naudio_reader = Audio(sample_rate=model.hparams.sample_rate)\nwaveform, sample_rate = audio_reader.crop(SAMPLE_WAV, SAMPLE_CHUNK)\n\n\n\n\n\n\n\noutput = model(waveform)\n\n\n\n\n\n\nThe model has accurately detected that two speakers are active (the orange one and the blue one) in this 5s chunk, and that they are partially overlapping around t=18s. The third speaker probability (in green) is close to zero for the whole five seconds.\n\n\n\n\n\npyannote.audio provides a convenient Inference class to apply the model using a 5s sliding window on the whole file:\n\nfrom pyannote.audio import Inference\ninference = Inference(model, duration=5.0, step=2.5)\noutput = inference(SAMPLE_WAV)\n\n\n\n\n\n\n\noutput.data.shape\n\n(11, 293, 3)\n\n\n\nBATCH_AXIS = 0\nTIME_AXIS = 1\nSPEAKER_AXIS = 2\n\nFor each of the 11 positions of the 5s window, the model outputs a 3-dimensional vector every 16ms (293 frames for 5 seconds), corresponding to the probabilities that each of (up to) 3 speakers is active."
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#he-who-can-do-more-can-do-less",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#he-who-can-do-more-can-do-less",
    "title": "One speaker segmentation model to rule them all",
    "section": "âHe who can do more can do lessâ",
    "text": "âHe who can do more can do lessâ\nThis model has more than one string to its bow and can prove useful for lots of speaker-related tasks such as:\n\nvoice activity detection\noverlapped speech detection\ninstantaneous speaker counting\nspeaker change detection\n\n\nVoice activity detection (VAD)\nVoice activity detection is the task of detecting speech regions in a given audio stream or recording.\nThis can be achieved by postprocessing the output of the model using the maximum over the speaker axis for each frame.\n\nto_vad = lambda o: np.max(o, axis=SPEAKER_AXIS, keepdims=True)\nto_vad(output)\n\n\n\n\nThe Inference class has a built-in mechanism to apply this transformation automatically on each window and aggregate the result using overlap-add. This is achieved by passing the above to_vad function via the pre_aggregation_hook parameter:\n\nvad = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_vad)\nvad_prob = vad(SAMPLE_WAV)\n\n\n\n\n\n\nBinarize utility class can eventually convert this frame-based probabiliy to the time domain:\n\nfrom pyannote.audio.utils.signal import Binarize\nbinarize = Binarize(onset=0.5)\nspeech = binarize(vad_prob)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped speech detection (OSD)\nOverlapped speech detection is the task of detecting regions where at least two speakers are speaking at the same time. This can be achieved by postprocessing the output of the model using the second maximum over the speaker axis for each frame.\n\nto_osd = lambda o: np.partition(o, -2, axis=SPEAKER_AXIS)[:, :, -2, np.newaxis]\nosd = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_osd)\nosd_prob = osd(SAMPLE_WAV)\n\n\n\n\n\n\n\nbinarize(osd_prob)\n\n\n\n\n\n\n\n\n\n\n\nInstantaneous speaker counting (CNT)\nInstantaneous speaker counting is a generalization of voice activity and overlapped speech detection that aims at returning the number of simultaneous speakers at each frame. This can be achieved by summing the output of the model over the speaker axis for each frame:\n\nto_cnt = lambda probability: np.sum(probability, axis=SPEAKER_AXIS, keepdims=True)\ncnt = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_cnt)\ncnt(SAMPLE_WAV)\n\n\n\n\n\n\nSpeaker change detection (SCD)\nSpeaker change detection is the task of detecting speaker change points in a given audio stream or recording. It can be achieved by taking the absolute value of the first derivative over the time axis, and take the maximum value over the speaker axis:\n\nto_scd = lambda probability: np.max(\n    np.abs(np.diff(probability, n=1, axis=TIME_AXIS)), \n    axis=SPEAKER_AXIS, keepdims=True)\nscd = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_scd)\nscd_prob = scd(SAMPLE_WAV)\n\n\n\n\n\n\nUsing a combination of Peak utility class (to detect peaks in the above curve) and voice activity detection output, we can obtain a decent segmentation into speaker turns:\n\nfrom pyannote.audio.utils.signal import Peak\npeak = Peak(alpha=0.05)\npeak(scd_prob).crop(speech.get_timeline())\n\n\n\n\n\n\n\n\n\n\n\nWhat pyannote/segmentation CANNOT do\nNow, letâs take a few steps back and have a closer look at the raw output of the model.\n\n\n\n\n\nDid you notice that the blue and orange speakers have been swapped between overlapping windows [15s, 20s] and [17.5s, 22.5s]?\nThis is a (deliberate) side effect of the permutation-invariant training process using when training the model.\nThe model is trained to discriminate speakers locally (i.e.Â within each window) but does not care about their global identity (i.e.Â at conversation scale). Have a look at this paper to learn more about this permutation-invariant training thing.\nThat means that this kind of model does not actually perform speaker diarization out of the box.\nLuckily, pyannote.audio has got you covered! pyannote/speaker-diarization pretrained pipeline uses pyannote/segmentation internally and combines it with speaker embeddings to deal with this permutation problem globally."
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#thats-all-folks",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#thats-all-folks",
    "title": "One speaker segmentation model to rule them all",
    "section": "Thatâs all folks!",
    "text": "Thatâs all folks!\nFor technical questions and bug reports, please check pyannote.audio Github repository.\nI also offer scientific consulting services around speaker diarization (and speech processing in general), please contact me if you think this type of technology might help your business/startup!"
  },
  {
    "objectID": "collab.html",
    "href": "collab.html",
    "title": "Collaborators",
    "section": "",
    "text": "Unsupervised and multimodal protolexicon discovery\nCo-supervised with Alejandrina Cristia (LSCP) and Emmanuel Dupoux (LSCP, FAIR)\n\n\n\n\n\nInteractive speaker diarization\nFull supervision"
  },
  {
    "objectID": "collab.html#current-phd-students",
    "href": "collab.html#current-phd-students",
    "title": "Collaborators",
    "section": "",
    "text": "Unsupervised and multimodal protolexicon discovery\nCo-supervised with Alejandrina Cristia (LSCP) and Emmanuel Dupoux (LSCP, FAIR)\n\n\n\n\n\nInteractive speaker diarization\nFull supervision"
  },
  {
    "objectID": "collab.html#former-phd-students",
    "href": "collab.html#former-phd-students",
    "title": "Collaborators",
    "section": "Former PhD students",
    "text": "Former PhD students\n\nJuan Manuel Coria (2019 - 2023)\n\nActive representation learning\nCo-supervised with Sophie Rosset (LISN) and Sahar Ghannay (LISN)\nNow - Machine Learning Researcher & Engineer at Ava\n\n\n\nLÃ©o Galmant (2018 - 2020)\n\nMachine learning for the automatic structuring of spoken conversations\nCo-supervised with Anne-Laure Ligozat (LISN) and Camille Guinaudeau (LISN)\n\n\n\nRuiqing Yin (2016-2019)\n\nSteps towards end-to-end neural speaker diarization\nCo-supervised with Claude Barras (LIMSI)\nNow - Research scientist at Huawei\n\n\n\nPhilipe Ercolessi (2010-2013)\n\nMultimodal extraction of the narrative structure of TV series episodes\nCo-supervised with Philippe Joly (IRIT) and Christine SÃ©nac (IRIT)\nNow - Head of research at Admo.tv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HervÃ© Bredin",
    "section": "",
    "text": "Deep learning for audio, speech and natural language processing\n\nTenured research scientist at French national institute for scientific research (CNRS)\nMember of the Institut de Recherche en Informatique de Toulouse (IRIT)\nMember of the SAMoVA team\n\nPublications: HAL or Google Scholar"
  },
  {
    "objectID": "index.html#academic-researcher",
    "href": "index.html#academic-researcher",
    "title": "HervÃ© Bredin",
    "section": "",
    "text": "Deep learning for audio, speech and natural language processing\n\nTenured research scientist at French national institute for scientific research (CNRS)\nMember of the Institut de Recherche en Informatique de Toulouse (IRIT)\nMember of the SAMoVA team\n\nPublications: HAL or Google Scholar"
  },
  {
    "objectID": "index.html#scientific-advisor",
    "href": "index.html#scientific-advisor",
    "title": "HervÃ© Bredin",
    "section": "Scientific advisor",
    "text": "Scientific advisor\nSelf-employed, available for contracting\n\nMachine learning\nDeep learning\nSpeech processing\nNatural language processing"
  },
  {
    "objectID": "index.html#open-source-developer",
    "href": "index.html#open-source-developer",
    "title": "HervÃ© Bredin",
    "section": "Open source developer",
    "text": "Open source developer\nReproducible research advocate\n\nLead developer of pyannote.audio  a deep learning toolkit for speaker diarization\nLead developer of pyannote.video  face detection, tracking and clustering in videos\nLead developer of pyannote.metrics  a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems\n\nAll projects: Github"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I received my PhD on talking-face biometric authentication from Telecom ParisTech (France) in 2007.\nIn 2008, I joined Dublin City University (Ireland) to work on video summarization techniques applied to user-generated content.\nI have been a permanent CNRS researcher since 2008:\n\nat IRIT (Toulouse, France) in the SAMoVA group until 2010\nat LIMSI (Orsay, France) in the Spoken Language Processing group until 2020\nback at IRIT (Toulouse, France) until today\n\nMy current research interests include deep learning techniques applied to machine listening, with a strong focus on speaker diarization: I am proud to be known as âthe guy behind pyannoteâ.\nSince 2021, I have also started working as a self-employed scientific advisor and am available for contracting on topics related to my academic research."
  },
  {
    "objectID": "about.html#biography",
    "href": "about.html#biography",
    "title": "About me",
    "section": "",
    "text": "I received my PhD on talking-face biometric authentication from Telecom ParisTech (France) in 2007.\nIn 2008, I joined Dublin City University (Ireland) to work on video summarization techniques applied to user-generated content.\nI have been a permanent CNRS researcher since 2008:\n\nat IRIT (Toulouse, France) in the SAMoVA group until 2010\nat LIMSI (Orsay, France) in the Spoken Language Processing group until 2020\nback at IRIT (Toulouse, France) until today\n\nMy current research interests include deep learning techniques applied to machine listening, with a strong focus on speaker diarization: I am proud to be known as âthe guy behind pyannoteâ.\nSince 2021, I have also started working as a self-employed scientific advisor and am available for contracting on topics related to my academic research."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About me",
    "section": "Contact",
    "text": "Contact\n\nPostal Address: IRIT, 118 Route de Narbonne, F-31062 Toulouse, Cedex, France\nOffice: IRIT1 219 (2nd floor)\nE-mail: herve.bredin@irit.fr\nTel: +33 (0)5 61 55 68 86"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "One speaker segmentation model to rule them all\n\n\n\n\n\nDeep dive into pyannote/segmentation pretrained model\n\n\n\n\n\n\nOct 23, 2022\n\n\nHervÃ© Bredin\n\n\n\n\n\n\n  \n\n\n\n\nStreaming voice activity detection with pyannote.audio\n\n\n\n\n\nHacking with streamz library for real-time voice activity detection\n\n\n\n\n\n\nAug 5, 2021\n\n\nHervÃ© Bredin\n\n\n\n\n\n\nNo matching items"
  }
]