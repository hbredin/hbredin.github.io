[
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "",
    "text": "In this blog post, I describe an early attempt at performing live voice activity detection with pyannote.audio pretrained segmentation model."
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#rolling-audio-buffer",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#rolling-audio-buffer",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Rolling audio buffer",
    "text": "Rolling audio buffer\nLet us assume that the audio stream is given as a 5s rolling buffer.\nHere, we are going to fake it by sliding a 5s window over the duration of an audio file.\n\nfrom pyannote.audio.core.io import Audio, AudioFile\n\nclass RollingAudioBuffer(Audio):\n    \"\"\"Rolling audio buffer\n    \n    Parameters\n    ----------\n    sample_rate : int\n        Sample rate\n    duration : float, optional\n        Duration of rolling buffer. Defaults to 5s.\n    step : float, optional\n        Delay between two updates of the rolling buffer. Defaults to 1s.\n\n\n    Usage\n    -----\n    &gt;&gt;&gt; buffer = RollingAudioBuffer()(\"audio.wav\")\n    &gt;&gt;&gt; current_buffer = next(buffer)\n    \"\"\"\n    def __init__(self, sample_rate=16000, duration=5.0, step=1.):\n        super().__init__(sample_rate=sample_rate, mono=True)\n        self.duration = duration\n        self.step = step\n        \n    def __call__(self, file: AudioFile):\n        \n        # duration of the whole audio file\n        duration = self.get_duration(file)\n        \n        # slide a 5s window from the beginning to the end of the file\n        window = SlidingWindow(start=0., duration=self.duration, step=self.step, end=duration)\n        for chunk in window:\n            # for each position of the window, yield the corresponding audio buffer\n            # as a SlidingWindowFeature instance\n            waveform, sample_rate = self.crop(file, chunk, duration=self.duration)\n            resolution = SlidingWindow(start=chunk.start, \n                                       duration=1./self.sample_rate, \n                                       step=1./sample_rate)\n            yield SWF(waveform.T, resolution)\n\nWe start by initializing rolling buffer on a sample file:\n\nMY_AUDIO_FILE = \"DH_0001.flac\"\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\n\nEach subsequent call to next(buffer) returns the current content of the 5s rolling buffer:\n\nnext(buffer)\n\n\n\n\n\nnext(buffer)\n\n\n\n\n\nnext(buffer)\n\n\n\n\nFor illustration purposes, we also load the manual voice activity reference.\n\nfrom pyannote.database.util import load_rttm\nreference = load_rttm('DH_0001.rttm').popitem()[1].get_timeline()\nreference"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#pretrained-voice-activity-detection-model",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#pretrained-voice-activity-detection-model",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Pretrained voice activity detection model",
    "text": "Pretrained voice activity detection model\npyannote.audio comes with a decent pretrained segmentation model that can be used for voice activity detection.\n\nimport torch\nimport numpy as np\nfrom pyannote.audio import Model\n\nclass VoiceActivityDetection:\n    \n    def __init__(self):\n        self.model = Model.from_pretrained(\"pyannote/segmentation\")\n        self.model.eval()\n        \n    def __call__(self, current_buffer: SWF) -&gt; SWF:\n        \n        # we start by applying the model on the current buffer\n        with torch.no_grad():\n            waveform = current_buffer.data.T\n            segmentation = self.model(waveform[np.newaxis]).numpy()[0]\n\n        # temporal resolution of the output of the model\n        resolution = self.model.introspection.frames\n        \n        # temporal shift to keep track of current buffer start time\n        resolution = SlidingWindow(start=current_buffer.sliding_window.start, \n                                   duration=resolution.duration, \n                                   step=resolution.step)\n            \n        # pyannote/segmentation pretrained model actually does more than just voice activity detection\n        # see https://huggingface.co/pyannote/segmentation for more details.     \n        speech_probability = np.max(segmentation, axis=-1, keepdims=True)\n        \n        return SWF(speech_probability, resolution)\n\n\nvad = VoiceActivityDetection()\n\nLet us try this thing on current buffer:\n\ncurrent_buffer = next(buffer)\ncurrent_buffer\n\n\n\n\n\nvad(current_buffer)\n\n\n\n\n\nreference"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#building-a-basic-streaming-pipeline-with-streamz",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#building-a-basic-streaming-pipeline-with-streamz",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Building a basic streaming pipeline with streamz",
    "text": "Building a basic streaming pipeline with streamz\nWe now have a way to stream audio and apply voice activity detection.\nAccording to its documentation, streamz seems like a good option to do that:\n\nStreamz helps you build pipelines to manage continuous streams of data.\n\nLet us start by creating a Stream that will ingest the rolling buffer and apply voice activity detection anytime the buffer is updated.\n\nfrom streamz import Stream\nsource = Stream()\nsource.map(vad).sink(visualize)\n\n\n\n\nWe re-initialize the audio buffer from the start of the file and push the rolling buffer into the pipeline:\n\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\nsource.emit(next(buffer))\n\n\n\n\n\nsource.emit(next(buffer))\n\n\n\n\n\nsource.emit(next(buffer))\n\n\n\n\n\nreference"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#controlling-latency-accuracy-trade-off",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#controlling-latency-accuracy-trade-off",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Controlling latency / accuracy trade-off",
    "text": "Controlling latency / accuracy trade-off\nThis is nice but we can do better in case the pipeline is allowed a small delay (a.k.a. latency) between when it receives the audio and when it outputs the voice activity detection scores.\nFor instance, if we are allowed 2s latency, we could benefit from the multiple overlapping buffers and combine them to get a better estimate of the speech probability in regions where the model is not quite confident (e.g.¬†just before t=4s).\nThis is what the Aggregation class does.\n\nfrom typing import Tuple, List\n\nclass Aggregation:\n    \"\"\"Aggregate multiple overlapping buffers with a \n    \n    Parameters\n    ----------\n    latency : float, optional\n        Allowed latency, in seconds. Defaults to 0.\n    \"\"\"\n    \n    def __init__(self, latency=0.0):\n        self.latency = latency\n        \n    def __call__(self, internal_state, current_buffer: SWF) -&gt; Tuple[Tuple[float, List[SWF]], SWF]:\n        \"\"\"Ingest new buffer and return aggregated output with delay\n\n        Parameters\n        ----------\n        internal_state : (internal_time, past_buffers) tuple\n            `internal_time` is a float such that previous call emitted aggregated scores up \n            to time `delayed_time`.  `past_buffers` is a rolling list of past buffers that \n            we are going to aggregate.\n        current_buffer : SlidingWindowFeature\n            New incoming score buffer.\n        \"\"\"\n\n        if internal_state is None:\n            internal_state = (0.0, list())\n        \n        # previous call led to the emission of aggregated scores up to time `delayed_time`\n        # `past_buffers` is a rolling list of past buffers that we are going to aggregate\n        delayed_time, past_buffers = internal_state\n        \n        # real time is the current end time of the audio buffer\n        # (here, estimated from the end time of the VAD buffer)\n        real_time = current_buffer.extent.end\n        \n        # because we are only allowed `self.latency` seconds of latency, this call should\n        # return aggregated scores for [delayed_time, real_time - latency] time range. \n        required = Segment(delayed_time, real_time - self.latency)\n        \n        # to compute more robust scores, we will combine all buffers that have a non-empty\n        # temporal intersection with required time range. we can get rid of the others as they\n        # will no longer be needed as they are too far away in the past.\n        past_buffers = [buffer for buffer in past_buffers if buffer.extent.end &gt; required.start] + [current_buffer]\n        \n        # we aggregate all past buffers (but only on the 'required' region of interest)\n        intersection = np.stack([buffer.crop(required, fixed=required.duration) for buffer in past_buffers])\n        aggregation = np.mean(intersection, axis=0)\n        \n        # ... and wrap it into a self-contained SlidingWindowFeature (SWF) instance\n        resolution = current_buffer.sliding_window\n        resolution = SlidingWindow(start=required.start, duration=resolution.duration, step=resolution.step)\n        output = SWF(aggregation, resolution)\n        \n        # we update the internal state\n        delayed_time = real_time - self.latency\n        internal_state = (delayed_time, past_buffers)\n        \n        # ... and return the whole thing for next call to know where we are\n        return internal_state, output\n\nLet‚Äôs add this new accumulator into the streaming pipeline, with a 2s latency:\n\nsource = Stream()\nsource \\\n    .map(vad) \\\n    .accumulate(Aggregation(latency=2.), returns_state=True, start=None) \\\n    .sink(visualize)\n\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\n\n\n# first [0s, 5s] buffer\ncurrent_buffer = next(buffer); current_buffer\n\n\n\n\n\n# ... and its corresponding output up to 3s (= 5s - latency)\nsource.emit(current_buffer)\n\n\n\n\n\n# next [1s, 6s] buffer\ncurrent_buffer = next(buffer); current_buffer\n\n\n\n\n\n# ... and its corresponding [3s, 6s - latency = 4s] output\nsource.emit(current_buffer)\n\n\n\n\n\n# next [2s, 7s] buffer\ncurrent_buffer = next(buffer); current_buffer\n\n\n\n\n\n# ... and its corresponding [4s, 7s - latency = 5s] output\nsource.emit(current_buffer)\n\n\n\n\nLook how the aggregation process actually refined the speech probability just before t=4s. This has been enabled by the longer latency."
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#thats-all-folks",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#thats-all-folks",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "That‚Äôs all folks!",
    "text": "That‚Äôs all folks!\nFor technical questions and bug reports, please check pyannote.audio Github repository.\nI also offer scientific consulting services around speaker diarization (and speech processing in general), please contact me if you think this type of technology might help your business/startup!"
  },
  {
    "objectID": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#bonus-concatenating-output",
    "href": "posts/2021-08-05-Streaming-voice-activity-detection-with-pyannote.html#bonus-concatenating-output",
    "title": "Streaming voice activity detection with pyannote.audio",
    "section": "Bonus: concatenating output",
    "text": "Bonus: concatenating output\nFor visualization purposes, you might want to add an accumulator to the pipeline that takes care of concatenating the output of each step‚Ä¶\n\nclass Concatenation:\n    \n    def __call__(self, concatenation: SWF, current_buffer: SWF) -&gt; Tuple[SWF, SWF]:\n        \n        if concatenation is None:\n            return current_buffer, current_buffer\n        \n        resolution = concatenation.sliding_window\n        current_start_frame = resolution.closest_frame(current_buffer.extent.start)\n        current_end_frame = current_start_frame + len(current_buffer)\n        \n        concatenation.data = np.pad(concatenation.data, ((0, current_end_frame - len(concatenation.data)), (0, 0)))\n        concatenation.data[current_start_frame: current_end_frame] = current_buffer.data        \n        \n        return concatenation, concatenation\n\n\nsource = Stream()\nsource \\\n    .map(vad) \\\n    .accumulate(Aggregation(latency=2.), returns_state=True, start=None) \\\n    .accumulate(Concatenation(), returns_state=True, start=None) \\\n    .sink(visualize)\n\nbuffer = RollingAudioBuffer()(MY_AUDIO_FILE)\n\n\nnotebook.crop = Segment(0, 30)\nfor _ in range(30):\n    source.emit(next(buffer))\n\n\n\n\n\nreference"
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html",
    "title": "One speaker segmentation model to rule them all",
    "section": "",
    "text": "In this blog post, I talk about pyannote.audio pretrained speaker segmentation model, which happens to be one of the most popular audio model available on ü§ó Huggingface model hub.\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\"pyannote/segmentation\")"
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#what-does-pyannotesegmentation-do",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#what-does-pyannotesegmentation-do",
    "title": "One speaker segmentation model to rule them all",
    "section": "What does pyannote/segmentation do?",
    "text": "What does pyannote/segmentation do?\nEvery pyannote.audio model has a specifications attribute that tells us a bit more about itself:\n\nprint(model.specifications)\n\nSpecifications(\n    problem=&lt;Problem.MULTI_LABEL_CLASSIFICATION: 2&gt;,\n    resolution=&lt;Resolution.FRAME: 1&gt;,\n    duration=5.0,\n    warm_up=(0.0, 0.0),\n    classes=['speaker#1', 'speaker#2', 'speaker#3'],\n    permutation_invariant=True\n)\n\n\n\nThese specifications tell us the following about pyannote/segmentation:\n\nit ingests audio chunks of 5 seconds duration\nit addresses a multi-label classification problem‚Ä¶\n‚Ä¶ whose possible classes are chosen among speaker#1, speaker#2, and speaker#3 ‚Ä¶\n‚Ä¶ and are permutation_invariant (more about that below)\n\nWe also learn that its output temporal resolution is the frame (i.e. it outputs a sequence of frame-wise decisions rather than just one decision for the whole chunk). The actual temporal resolution can be obtained through the magic introspection attribute (approximately 17ms for pyannote/segmentation):\n\nmodel.introspection.frames.step\n\n0.016875"
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#ok-but-what-does-pyannotesegmentation-really-do",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#ok-but-what-does-pyannotesegmentation-really-do",
    "title": "One speaker segmentation model to rule them all",
    "section": "OK, but what does pyannote/segmentation really do?",
    "text": "OK, but what does pyannote/segmentation really do?\nTo answer this question, let us consider the audio recording of a 30s conversation between two speakers (the blue one and the red one):\n\n\n\n\n\n\n\n\n                \n                    \n                    Your browser does not support the audio element.\n                \n              \n\n\nLet‚Äôs apply the model on this 5s excerpt of the conversation:\n\nfrom pyannote.audio import Audio\naudio_reader = Audio(sample_rate=model.hparams.sample_rate)\nwaveform, sample_rate = audio_reader.crop(SAMPLE_WAV, SAMPLE_CHUNK)\n\n\n\n\n\n\n\noutput = model(waveform)\n\n\n\n\n\n\nThe model has accurately detected that two speakers are active (the orange one and the blue one) in this 5s chunk, and that they are partially overlapping around t=18s. The third speaker probability (in green) is close to zero for the whole five seconds.\n\n\n\n\n\npyannote.audio provides a convenient Inference class to apply the model using a 5s sliding window on the whole file:\n\nfrom pyannote.audio import Inference\ninference = Inference(model, duration=5.0, step=2.5)\noutput = inference(SAMPLE_WAV)\n\n\n\n\n\n\n\noutput.data.shape\n\n(11, 293, 3)\n\n\n\nBATCH_AXIS = 0\nTIME_AXIS = 1\nSPEAKER_AXIS = 2\n\nFor each of the 11 positions of the 5s window, the model outputs a 3-dimensional vector every 16ms (293 frames for 5 seconds), corresponding to the probabilities that each of (up to) 3 speakers is active."
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#he-who-can-do-more-can-do-less",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#he-who-can-do-more-can-do-less",
    "title": "One speaker segmentation model to rule them all",
    "section": "‚ÄúHe who can do more can do less‚Äù",
    "text": "‚ÄúHe who can do more can do less‚Äù\nThis model has more than one string to its bow and can prove useful for lots of speaker-related tasks such as:\n\nvoice activity detection\noverlapped speech detection\ninstantaneous speaker counting\nspeaker change detection\n\n\nVoice activity detection (VAD)\nVoice activity detection is the task of detecting speech regions in a given audio stream or recording.\nThis can be achieved by postprocessing the output of the model using the maximum over the speaker axis for each frame.\n\nto_vad = lambda o: np.max(o, axis=SPEAKER_AXIS, keepdims=True)\nto_vad(output)\n\n\n\n\nThe Inference class has a built-in mechanism to apply this transformation automatically on each window and aggregate the result using overlap-add. This is achieved by passing the above to_vad function via the pre_aggregation_hook parameter:\n\nvad = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_vad)\nvad_prob = vad(SAMPLE_WAV)\n\n\n\n\n\n\nBinarize utility class can eventually convert this frame-based probabiliy to the time domain:\n\nfrom pyannote.audio.utils.signal import Binarize\nbinarize = Binarize(onset=0.5)\nspeech = binarize(vad_prob)\n\n\n\n\n\n\n\n\n\n\n\n\n\nOverlapped speech detection (OSD)\nOverlapped speech detection is the task of detecting regions where at least two speakers are speaking at the same time. This can be achieved by postprocessing the output of the model using the second maximum over the speaker axis for each frame.\n\nto_osd = lambda o: np.partition(o, -2, axis=SPEAKER_AXIS)[:, :, -2, np.newaxis]\nosd = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_osd)\nosd_prob = osd(SAMPLE_WAV)\n\n\n\n\n\n\n\nbinarize(osd_prob)\n\n\n\n\n\n\n\n\n\n\n\nInstantaneous speaker counting (CNT)\nInstantaneous speaker counting is a generalization of voice activity and overlapped speech detection that aims at returning the number of simultaneous speakers at each frame. This can be achieved by summing the output of the model over the speaker axis for each frame:\n\nto_cnt = lambda probability: np.sum(probability, axis=SPEAKER_AXIS, keepdims=True)\ncnt = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_cnt)\ncnt(SAMPLE_WAV)\n\n\n\n\n\n\nSpeaker change detection (SCD)\nSpeaker change detection is the task of detecting speaker change points in a given audio stream or recording. It can be achieved by taking the absolute value of the first derivative over the time axis, and take the maximum value over the speaker axis:\n\nto_scd = lambda probability: np.max(\n    np.abs(np.diff(probability, n=1, axis=TIME_AXIS)), \n    axis=SPEAKER_AXIS, keepdims=True)\nscd = Inference(\"pyannote/segmentation\", pre_aggregation_hook=to_scd)\nscd_prob = scd(SAMPLE_WAV)\n\n\n\n\n\n\nUsing a combination of Peak utility class (to detect peaks in the above curve) and voice activity detection output, we can obtain a decent segmentation into speaker turns:\n\nfrom pyannote.audio.utils.signal import Peak\npeak = Peak(alpha=0.05)\npeak(scd_prob).crop(speech.get_timeline())\n\n\n\n\n\n\n\n\n\n\n\nWhat pyannote/segmentation CANNOT do\nNow, let‚Äôs take a few steps back and have a closer look at the raw output of the model.\n\n\n\n\n\nDid you notice that the blue and orange speakers have been swapped between overlapping windows [15s, 20s] and [17.5s, 22.5s]?\nThis is a (deliberate) side effect of the permutation-invariant training process using when training the model.\nThe model is trained to discriminate speakers locally (i.e.¬†within each window) but does not care about their global identity (i.e.¬†at conversation scale). Have a look at this paper to learn more about this permutation-invariant training thing.\nThat means that this kind of model does not actually perform speaker diarization out of the box.\nLuckily, pyannote.audio has got you covered! pyannote/speaker-diarization pretrained pipeline uses pyannote/segmentation internally and combines it with speaker embeddings to deal with this permutation problem globally."
  },
  {
    "objectID": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#thats-all-folks",
    "href": "posts/2022-10-23-One-speaker-segmentation-model-to-rule-them-all.html#thats-all-folks",
    "title": "One speaker segmentation model to rule them all",
    "section": "That‚Äôs all folks!",
    "text": "That‚Äôs all folks!\nFor technical questions and bug reports, please check pyannote.audio Github repository.\nI also offer scientific consulting services around speaker diarization (and speech processing in general), please contact me if you think this type of technology might help your business/startup!"
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "",
    "text": "pyannote.audio is an open-source toolkit written in Python for speaker diarization.\nVersion 2.1 introduces a major overhaul of the default speaker diarization pipeline, made of three main stages: speaker segmentation applied to a short sliding window, neural speaker embedding of each (local) speakers, and (global) agglomerative clustering.\nDespite its decent out-of-the-box performance, the default pipeline may suffer from the domain mismatch problem (common to most machine learning models) and not perform well on your own data. This blog post will guide you through two recipes to adapt it to your own data and (hopefully) get better performance. Depending on the number and duration of labeled conversations, you may either focus on optimizing hyper-parameters or additionally fine-tune the internal speaker segmentation model."
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#installation",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#installation",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "Installation",
    "text": "Installation\nLet‚Äôs start by installing pyannote.audio 2.1.1 (and rich for pretty progress bars).\n\n!pip install -qq pyannote.audio==2.1.1\n!pip install -qq rich"
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#data-preparation",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#data-preparation",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "Data preparation",
    "text": "Data preparation\nFirst things first: we need data‚Ä¶ Annotated data! Ideally, lots of annotated data!\nFor the purpose of this blog post, we will rely on the AMI-SDM (single distance microphone) corpus.\n\n# download AMI-SDM mini corpus\n%cd /content/\n!git clone https://github.com/pyannote/AMI-diarization-setup\n%cd /content/AMI-diarization-setup/pyannote/\n!bash download_ami_sdm_mini.sh\n\n\n!PYANNOTE_DATABASE_CONFIG=\"/content/AMI-diarization-setup/pyannote/database.yml\" pyannote-database info AMI-SDM.SpeakerDiarization.mini\n\ntrain\n   28 files\n   8h46m annotated\n   6h11m of speech (71%)\n   112 speakers\ndevelopment\n   3 files\n   0h56m annotated\n   0h40m of speech (72%)\n   12 speakers\ntest\n   3 files\n   0h56m annotated\n   0h39m of speech (70%)\n   12 speakers\n\n\nNote that we use a ‚Äúmini‚Äù version of AMI-SDM so that the code in this blog post can be run in half an hour but the full version is also available for you to get better results.\nIf you want to try it, replace download_ami_sdm_mini.sh by download_ami_sdm.sh and AMI-SDM.SpeakerDiarization.mini by AMI-SDM.SpeakerDiarization.only_words and you are good to go!\n\nfrom pyannote.database import registry, FileFinder\nregistry.load_database(\"/content/AMI-diarization-setup/pyannote/database.yml\")\ndataset = registry.get_protocol(\n    \"AMI-SDM.SpeakerDiarization.mini\", \n    {\"audio\": FileFinder()}\n)"
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#pretrained-pipeline",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#pretrained-pipeline",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "Pretrained pipeline",
    "text": "Pretrained pipeline\nLet‚Äôs start by running the pretrained pipeline on the test set and evaluate its performance.\nOfficial pyannote.audio pipelines (i.e.¬†those under the pyannote organization umbrella) are open-source, but gated. It means that you have to first accept users conditions on their respective Huggingface page to access the pretrained weights and hyper-parameters.\nTo load the speaker diarization pipeline used in this blog post, you have to\n\nvisit hf.co/pyannote/speaker-diarization and accept the terms\nvisit hf.co/pyannote/segmentation (used internally by the speaker diarization pipeline)and accept the terms\nlog in using notebook_login\n\n\nfrom huggingface_hub import notebook_login\nnotebook_login()\n\n\nfrom pyannote.audio import Pipeline\npretrained_pipeline = Pipeline.from_pretrained(\n    \"pyannote/speaker-diarization\", use_auth_token=True) \n\n\n# this takes approximately 2min to run on Google Colab GPU\nfrom pyannote.metrics.diarization import DiarizationErrorRate\nmetric = DiarizationErrorRate()\n\nfor file in dataset.test():\n    # apply pretrained pipeline\n    file[\"pretrained pipeline\"] = pretrained_pipeline(file)\n\n    # evaluate its performance\n    metric(file[\"annotation\"], file[\"pretrained pipeline\"], uem=file[\"annotated\"])\n\nprint(f\"The pretrained pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")\n\nThe pretrained pipeline reaches a Diarization Error Rate (DER) of 32.5% on AMI-SDM.SpeakerDiarization.mini test set.\n\n\n\nfile[\"annotation\"]\n\n\n\n\n\nfile[\"pretrained pipeline\"]"
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#fine-tuning-the-segmentation-model",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#fine-tuning-the-segmentation-model",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "Fine-tuning the segmentation model",
    "text": "Fine-tuning the segmentation model\nWhen a sufficiently large training set of labeled conversations is available, fine-tuning the internal speaker segmentation model may lead to significant performance boost.\nStarting from the pretrained model‚Ä¶\n\nfrom pyannote.audio import Model\nmodel = Model.from_pretrained(\"pyannote/segmentation\", use_auth_token=True)\n\n‚Ä¶ we prepare it for fine-tuning on the training dataset:\n\nfrom pyannote.audio.tasks import Segmentation\ntask = Segmentation(\n    dataset, \n    duration=model.specifications.duration, \n    max_num_speakers=len(model.specifications.classes), \n    batch_size=32,\n    num_workers=2, \n    loss=\"bce\", \n    vad_loss=\"bce\")\nmodel.task = task\nmodel.setup(stage=\"fit\")\n\nThe actual training is done with lightning:\n\n# this takes approximately 15min to run on Google Colab GPU\nfrom types import MethodType\nfrom torch.optim import Adam\nfrom pytorch_lightning.callbacks import (\n    EarlyStopping,\n    ModelCheckpoint,\n    RichProgressBar,\n)\n\n# we use Adam optimizer with 1e-4 learning rate\ndef configure_optimizers(self):\n    return Adam(self.parameters(), lr=1e-4)\n\nmodel.configure_optimizers = MethodType(configure_optimizers, model)\n\n# we monitor diarization error rate on the validation set\n# and use to keep the best checkpoint and stop early\nmonitor, direction = task.val_monitor\ncheckpoint = ModelCheckpoint(\n    monitor=monitor,\n    mode=direction,\n    save_top_k=1,\n    every_n_epochs=1,\n    save_last=False,\n    save_weights_only=False,\n    filename=\"{epoch}\",\n    verbose=False,\n)\nearly_stopping = EarlyStopping(\n    monitor=monitor,\n    mode=direction,\n    min_delta=0.0,\n    patience=10,\n    strict=True,\n    verbose=False,\n)\n\ncallbacks = [RichProgressBar(), checkpoint, early_stopping]\n\n# we train for at most 20 epochs (might be shorter in case of early stopping)\nfrom pytorch_lightning import Trainer\ntrainer = Trainer(accelerator=\"gpu\", \n                  callbacks=callbacks, \n                  max_epochs=20,\n                  gradient_clip_val=0.5)\ntrainer.fit(model)\n\n\n# save path to the best checkpoint for later use\nfinetuned_model = checkpoint.best_model_path"
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#optimizing-the-pipeline-hyper-parameters",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#optimizing-the-pipeline-hyper-parameters",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "Optimizing the pipeline hyper-parameters",
    "text": "Optimizing the pipeline hyper-parameters\nThe pretrained pyannote/speaker-diarization pipeline relies on its own set of hyper-parameters adapted to the internal pyannote/segmentation pretrained model:\n\npretrained_hyperparameters = pretrained_pipeline.parameters(instantiated=True)\npretrained_hyperparameters\n\n{'segmentation': {'min_duration_off': 0.5817029604921046,\n  'threshold': 0.4442333667381752},\n 'clustering': {'method': 'centroid',\n  'min_cluster_size': 15,\n  'threshold': 0.7153814381597874}}\n\n\nThere is no reason the above hyper-parameters are optimal for the newly finetuned speaker segmentation model. Let‚Äôs optimize them:\n\nsegmentation.threshold (\\(\\theta\\) in the technical report, between 0 and 1) controls the aggressiveness of speaker activity detection (i.e.¬†a higher value will result in less detected speech);\nclustering.threshold (\\(\\delta\\) in the report, between 0 and 2) controls the number of speakers (i.e.¬†a higher value will result in less speakers).\nsegmentation.min_duration_off (\\(\\Delta\\) in the report, in seconds) controls whether intra-speaker pauses are filled. This usually depends on the downstream application so it is better to first force it to zero (i.e.¬†never fill intra-speaker pauses) during optimization.\nclustering.centroid is the linkage used by the agglomerative clustering step. centroid has been found to be slightly better than average.\nclustering.min_cluster_size controls what to do with small speaker clusters. Clusters smaller than that are assigned to the most similar large cluster. 15 is a good default value.\n\nWe start by optimizing segmentation.threshold by assuming that the subsequent clustering step is perfect (cf.¬†OracleClustering).\n\n# this takes approximately 5min to run on Google Colab GPU\nfrom pyannote.audio.pipelines import SpeakerDiarization\nfrom pyannote.pipeline import Optimizer\n\npipeline = SpeakerDiarization(\n    segmentation=finetuned_model,\n    clustering=\"OracleClustering\",  \n)\n# as reported in the technical report, min_duration_off can safely be set to 0.0\npipeline.freeze({\"segmentation\": {\"min_duration_off\": 0.0}})\n\noptimizer = Optimizer(pipeline)\ndev_set = list(dataset.development())\n\niterations = optimizer.tune_iter(dev_set, show_progress=False)\nbest_loss = 1.0\nfor i, iteration in enumerate(iterations):\n    print(f\"Best segmentation threshold so far: {iteration['params']['segmentation']['threshold']}\")\n    if i &gt; 20: break   # 50 iterations should give slightly better results\n\nThen, we use the optimized value of segmentation.threshold and optimize clustering.threshold.\n\nbest_segmentation_threshold = optimizer.best_params[\"segmentation\"][\"threshold\"]\n\n\n# this takes approximately 5min to run on Google Colab GPU\npipeline = SpeakerDiarization(\n    segmentation=finetuned_model,\n    embedding=pretrained_pipeline.embedding,\n    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n    clustering=pretrained_pipeline.klustering,\n)\n\npipeline.freeze({\n    \"segmentation\": {\n        \"threshold\": best_segmentation_threshold,\n        \"min_duration_off\": 0.0,\n    },\n    \"clustering\": {\n        \"method\": \"centroid\",\n        \"min_cluster_size\": 15,\n    },\n})\n\noptimizer = Optimizer(pipeline)\niterations = optimizer.tune_iter(dev_set, show_progress=False)\nbest_loss = 1.0\nfor i, iteration in enumerate(iterations):\n    print(f\"Best clustering threshold so far: {iteration['params']['clustering']['threshold']}\")\n    if i &gt; 20: break  # 50 iterations should give slightly better results\n\nFinally, we use the optimized values of segmentation.threshold and clustering.threshold to evaluate the performance of the finetuned pipeline:\n\nbest_clustering_threshold = optimizer.best_params['clustering']['threshold']\n\n\n# this takes approximately 2min to run on Google Colab GPU\nfinetuned_pipeline = SpeakerDiarization(\n    segmentation=finetuned_model,\n    embedding=pretrained_pipeline.embedding,\n    embedding_exclude_overlap=pretrained_pipeline.embedding_exclude_overlap,\n    clustering=pretrained_pipeline.klustering,\n)\n\nfinetuned_pipeline.instantiate({\n    \"segmentation\": {\n        \"threshold\": best_segmentation_threshold,\n        \"min_duration_off\": 0.0,\n    },\n    \"clustering\": {\n        \"method\": \"centroid\",\n        \"min_cluster_size\": 15,\n        \"threshold\": best_clustering_threshold,\n    },\n})\n\nmetric = DiarizationErrorRate()\n\nfor file in dataset.test():\n    # apply finetuned pipeline\n    file[\"finetuned pipeline\"] = finetuned_pipeline(file)\n\n    # evaluate its performance\n    metric(file[\"annotation\"], file[\"finetuned pipeline\"], uem=file[\"annotated\"])\n\nprint(f\"The finetuned pipeline reaches a Diarization Error Rate (DER) of {100 * abs(metric):.1f}% on {dataset.name} test set.\")\n\nThe finetuned pipeline reaches a Diarization Error Rate (DER) of 26.6% on AMI-SDM.SpeakerDiarization.mini test set.\n\n\n\nfile[\"finetuned pipeline\"]\n\n\n\n\n\nfile[\"annotation\"]"
  },
  {
    "objectID": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#conclusion",
    "href": "posts/2022-12-02-how-I-won-2022-diarization-challenges.html#conclusion",
    "title": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data",
    "section": "Conclusion",
    "text": "Conclusion\nIn just about half an hour (and 6 hours of training data), we managed to reduce the diarization error rate from 32.5% to 26.6%.\nYours truly used this very recipe for their submissions to several speaker diarization benchmarks organized in 2022. I reached:\n\n6th place at VoxSRC 2022 speaker diarization challenge\n1st place at Ego4D 2022 audio-only speaker diarization challenge\n1st place at Albayzin 2022 speaker diarization challenge\n\nThe technical report contains a detailed description of the pipeline, as well as an extensive evaluation of its performance on multiple benchmarking datasets.\nFor technical questions and bug reports, please check pyannote.audio Github repository so that my (or anyone‚Äôs) public answer benefits other people as well.\nI also offer scientific consulting services around speaker diarization (and speech processing in general), please contact me if you think this type of technology might help your business/startup!"
  },
  {
    "objectID": "collab.html",
    "href": "collab.html",
    "title": "Collaborators",
    "section": "",
    "text": "Unsupervised and multimodal protolexicon discovery\nCo-supervised with Alejandrina Cristia (LSCP) and Emmanuel Dupoux (LSCP, FAIR)\n\n\n\n\n\nInteractive speaker diarization\nFull supervision"
  },
  {
    "objectID": "collab.html#current-phd-students",
    "href": "collab.html#current-phd-students",
    "title": "Collaborators",
    "section": "",
    "text": "Unsupervised and multimodal protolexicon discovery\nCo-supervised with Alejandrina Cristia (LSCP) and Emmanuel Dupoux (LSCP, FAIR)\n\n\n\n\n\nInteractive speaker diarization\nFull supervision"
  },
  {
    "objectID": "collab.html#former-phd-students",
    "href": "collab.html#former-phd-students",
    "title": "Collaborators",
    "section": "Former PhD students",
    "text": "Former PhD students\n\nJuan Manuel Coria (2019 - 2023)\n\nActive representation learning\nCo-supervised with Sophie Rosset (LISN) and Sahar Ghannay (LISN)\nNow - Machine Learning Researcher & Engineer at Ava\n\n\n\nL√©o Galmant (2018 - 2020)\n\nMachine learning for the automatic structuring of spoken conversations\nCo-supervised with Anne-Laure Ligozat (LISN) and Camille Guinaudeau (LISN)\n\n\n\nRuiqing Yin (2016-2019)\n\nSteps towards end-to-end neural speaker diarization\nCo-supervised with Claude Barras (LIMSI)\nNow - Research scientist at Huawei\n\n\n\nPhilipe Ercolessi (2010-2013)\n\nMultimodal extraction of the narrative structure of TV series episodes\nCo-supervised with Philippe Joly (IRIT) and Christine S√©nac (IRIT)\nNow - Head of research at Admo.tv"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Herv√© Bredin",
    "section": "",
    "text": "Deep learning for audio, speech and natural language processing\n\nTenured research scientist at French national institute for scientific research (CNRS)\nMember of the Institut de Recherche en Informatique de Toulouse (IRIT)\nMember of the SAMoVA team\n\nPublications: HAL or Google Scholar"
  },
  {
    "objectID": "index.html#academic-researcher",
    "href": "index.html#academic-researcher",
    "title": "Herv√© Bredin",
    "section": "",
    "text": "Deep learning for audio, speech and natural language processing\n\nTenured research scientist at French national institute for scientific research (CNRS)\nMember of the Institut de Recherche en Informatique de Toulouse (IRIT)\nMember of the SAMoVA team\n\nPublications: HAL or Google Scholar"
  },
  {
    "objectID": "index.html#scientific-advisor",
    "href": "index.html#scientific-advisor",
    "title": "Herv√© Bredin",
    "section": "Scientific advisor",
    "text": "Scientific advisor\nSelf-employed, available for contracting\n\nMachine learning\nDeep learning\nSpeech processing\nNatural language processing"
  },
  {
    "objectID": "index.html#open-source-developer",
    "href": "index.html#open-source-developer",
    "title": "Herv√© Bredin",
    "section": "Open source developer",
    "text": "Open source developer\nReproducible research advocate\n\nLead developer of pyannote.audio  a deep learning toolkit for speaker diarization\nLead developer of pyannote.video  face detection, tracking and clustering in videos\nLead developer of pyannote.metrics  a toolkit for reproducible evaluation, diagnostic, and error analysis of speaker diarization systems\n\nAll projects: Github"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "I received my PhD on talking-face biometric authentication from Telecom ParisTech (France) in 2007.\nIn 2008, I joined Dublin City University (Ireland) to work on video summarization techniques applied to user-generated content.\nI have been a permanent CNRS researcher since 2008:\n\nat IRIT (Toulouse, France) in the SAMoVA group until 2010\nat LIMSI (Orsay, France) in the Spoken Language Processing group until 2020\nback at IRIT (Toulouse, France) until today\n\nMy current research interests include deep learning techniques applied to machine listening, with a strong focus on speaker diarization: I am proud to be known as ‚Äúthe guy behind pyannote‚Äù.\nSince 2021, I have also started working as a self-employed scientific advisor and am available for contracting on topics related to my academic research."
  },
  {
    "objectID": "about.html#biography",
    "href": "about.html#biography",
    "title": "About me",
    "section": "",
    "text": "I received my PhD on talking-face biometric authentication from Telecom ParisTech (France) in 2007.\nIn 2008, I joined Dublin City University (Ireland) to work on video summarization techniques applied to user-generated content.\nI have been a permanent CNRS researcher since 2008:\n\nat IRIT (Toulouse, France) in the SAMoVA group until 2010\nat LIMSI (Orsay, France) in the Spoken Language Processing group until 2020\nback at IRIT (Toulouse, France) until today\n\nMy current research interests include deep learning techniques applied to machine listening, with a strong focus on speaker diarization: I am proud to be known as ‚Äúthe guy behind pyannote‚Äù.\nSince 2021, I have also started working as a self-employed scientific advisor and am available for contracting on topics related to my academic research."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About me",
    "section": "Contact",
    "text": "Contact\n\nPostal Address: IRIT, 118 Route de Narbonne, F-31062 Toulouse, Cedex, France\nOffice: IRIT1 219 (2nd floor)\nE-mail: herve.bredin@irit.fr\nTel: +33 (0)5 61 55 68 86"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Blog",
    "section": "",
    "text": "Adapting pyannote.audio 2.1 pretrained speaker diarization pipeline to your own data\n\n\n\n\n\nHow I reached 1st place at Ego4D 2022, 1st place at Albayzin 2022, and 6th place at VoxSRC 2022 speaker diarization challenges\n\n\n\n\n\n\nDec 2, 2022\n\n\nHerv√© Bredin\n\n\n\n\n\n\n  \n\n\n\n\nOne speaker segmentation model to rule them all\n\n\n\n\n\nDeep dive into pyannote/segmentation pretrained model\n\n\n\n\n\n\nOct 23, 2022\n\n\nHerv√© Bredin\n\n\n\n\n\n\n  \n\n\n\n\nStreaming voice activity detection with pyannote.audio\n\n\n\n\n\nHacking with streamz library for real-time voice activity detection\n\n\n\n\n\n\nAug 5, 2021\n\n\nHerv√© Bredin\n\n\n\n\n\n\nNo matching items"
  }
]